{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c6c2d3-652d-453b-90a1-15f6dbe3413a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Databricks Widgets\n",
    "# Used to parameterize the notebook for Jobs / reusability\n",
    "# ------------------------------------------------------------\n",
    "dbutils.widgets.text(\"s3_source_path\", \"\", \"S3 Source URI (CSV)\")\n",
    "dbutils.widgets.text(\"s3_target_path\", \"\", \"S3 Target URI (Parquet)\")\n",
    "dbutils.widgets.text(\"start_year\", \"\", \"Start Year\")\n",
    "dbutils.widgets.text(\"end_year\", \"\", \"End Year\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Target Table Name\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"movielens\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"bronze\", \"Schema Name\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Read widget values\n",
    "# ------------------------------------------------------------\n",
    "s3_source_path = dbutils.widgets.get(\"s3_source_path\")\n",
    "s3_target_path = dbutils.widgets.get(\"s3_target_path\")\n",
    "start_year = dbutils.widgets.get(\"start_year\")\n",
    "end_year = dbutils.widgets.get(\"end_year\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8bd184-236d-4c6c-a575-31150c0344a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Input validation\n",
    "# Fail fast for misconfigured jobs\n",
    "# ------------------------------------------------------------\n",
    "if not s3_source_path or not s3_source_path.startswith(\"s3://\"):\n",
    "    raise ValueError(f\"CONFIGURATION ERROR: Invalid source path '{s3_source_path}'\")\n",
    "\n",
    "if not s3_target_path or not s3_target_path.startswith(\"s3://\"):\n",
    "    raise ValueError(f\"CONFIGURATION ERROR: Invalid target path '{s3_target_path}'\")\n",
    "\n",
    "if not start_year:\n",
    "    raise ValueError(\"CONFIGURATION ERROR: start_year not passed\")\n",
    "\n",
    "if not end_year:\n",
    "    raise ValueError(\"CONFIGURATION ERROR: end_year not passed\")\n",
    "\n",
    "if not table_name:\n",
    "    raise ValueError(\"CONFIGURATION ERROR: table_name not passed\")\n",
    "\n",
    "# Normalize S3 paths to avoid malformed file paths\n",
    "if not s3_source_path.endswith(\"/\"):\n",
    "    s3_source_path += \"/\"\n",
    "\n",
    "if not s3_target_path.endswith(\"/\"):\n",
    "    s3_target_path += \"/\"\n",
    "\n",
    "print(\"[INFO] Validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c9118a8-6e2d-4d10-b3dc-39fa0113359d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Explicit schemas for Bronze ingestion\n",
    "# Schema inference is intentionally avoided\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, DoubleType, LongType, StringType\n",
    ")\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "tags_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "schemas = {\n",
    "    \"ratings\": ratings_schema,\n",
    "    \"tags\": tags_schema\n",
    "}\n",
    "\n",
    "# Validate supported table names\n",
    "if table_name not in schemas:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported table_name '{table_name}'. \"\n",
    "        f\"Supported: {list(schemas.keys())}\"\n",
    "    )\n",
    "\n",
    "# Fully qualified Unity Catalog table name\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(f\"[START] Job Initialized for table {full_table_name}\")\n",
    "print(f\"Source: {s3_source_path}\")\n",
    "print(f\"Target: {s3_target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08d001d9-55b2-432d-80a4-c93ade2aec97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Historical ingestion logic\n",
    "# Iterates year-wise and appends data to Bronze Delta table\n",
    "# ------------------------------------------------------------\n",
    "def process_historical_data():\n",
    "    total_files = 0\n",
    "\n",
    "    for year in range(int(start_year), int(end_year) + 1):\n",
    "        source_file = f\"{s3_source_path}{table_name}_{year}.csv\"\n",
    "        print(f\"Processing Year: {year} | Source: {source_file}\")\n",
    "\n",
    "        try:\n",
    "            df = (\n",
    "                spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .schema(schemas[table_name])\n",
    "                    .load(source_file)\n",
    "            )\n",
    "\n",
    "            (\n",
    "                df.write\n",
    "                    .format(\"delta\")\n",
    "                    .mode(\"append\")  # Historical loads are appended year by year\n",
    "                    .save(s3_target_path)\n",
    "            )\n",
    "\n",
    "            total_files += 1\n",
    "            print(f\"Appended data for year {year}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Missing or corrupt yearly files are skipped intentionally\n",
    "            print(f\"Skipping year {year}: {e}\")\n",
    "\n",
    "    return total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b0bb2a1-80d2-40ad-a3e5-1ae0c4f4fd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Execute ingestion\n",
    "# ------------------------------------------------------------\n",
    "files_processed = process_historical_data()\n",
    "\n",
    "expected_files = int(end_year) - int(start_year) + 1\n",
    "if files_processed < expected_files:\n",
    "    print(\n",
    "        f\"WARNING: Expected {expected_files} files, \"\n",
    "        f\"but processed only {files_processed}\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Register Delta table in Unity Catalog\n",
    "# Storage and metadata are intentionally decoupled\n",
    "# ------------------------------------------------------------\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {full_table_name}\n",
    "    USING DELTA\n",
    "    LOCATION '{s3_target_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"[END] Job completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "historical_data_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "471a3359-edbc-4830-83f1-7a65c6301be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATABRICKS NOTEBOOK: 01_Ingest_Static_Bronze\n",
    "# ==============================================================================\n",
    "# DESCRIPTION:\n",
    "#   Generic ingestion utility to load raw CSV data from S3 into the Bronze Layer\n",
    "#   of the Delta Lakehouse. This script implements the \"External Table\" pattern,\n",
    "#   decoupling storage (S3) from metadata (Unity Catalog).\n",
    "#\n",
    "# USAGE:\n",
    "#   Designed to be triggered via Databricks Workflows or Airflow.\n",
    "#   Requires the following parameters to be passed at runtime.\n",
    "#\n",
    "# PARAMETERS:\n",
    "#   - s3_source_path: Full S3 URI of the source CSV file (e.g., s3://bucket/raw/file.csv)\n",
    "#   - s3_target_path: Target S3 URI where Delta files will be stored.\n",
    "#   - table_name:     The logical name of the table in the Catalog.\n",
    "#   - catalog_name:   Target Unity Catalog name (Default: movielens)\n",
    "#   - schema_name:    Target Schema name (Default: bronze)\n",
    "# ==============================================================================\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & WIDGET DEFINITION\n",
    "# -------------------------------------------------------------------------\n",
    "# Define input widgets to accept parameters from the Orchestrator (Airflow/Jobs).\n",
    "dbutils.widgets.text(\"s3_source_path\", \"\", \"Source S3 URI (CSV)\")\n",
    "dbutils.widgets.text(\"s3_target_path\", \"\", \"Target S3 URI (Delta Location)\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Target Table Name\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"movielens\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema_name\", \"bronze\", \"Schema\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. PARAMETER RETRIEVAL & VALIDATION\n",
    "# -------------------------------------------------------------------------\n",
    "# Fetch values from the runtime context\n",
    "s3_source_path = dbutils.widgets.get(\"s3_source_path\")\n",
    "s3_target_path = dbutils.widgets.get(\"s3_target_path\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "# Construct the fully qualified table name (Three-Level Namespace)\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# Guardrails: Ensure strict adherence to architectural standards (S3 Only)\n",
    "if not s3_source_path.startswith(\"s3://\"):\n",
    "    raise ValueError(f\"CONFIGURATION ERROR: Invalid Source Path '{s3_source_path}'. Must be a valid s3:// URI.\")\n",
    "\n",
    "if not s3_target_path.startswith(\"s3://\"):\n",
    "    raise ValueError(f\"CONFIGURATION ERROR: Invalid Target Path '{s3_target_path}'. Must be a valid s3:// URI.\")\n",
    "\n",
    "print(f\"[START] Job initialized for table: {full_table_name}\")\n",
    "print(f\"Source: {s3_source_path}\")\n",
    "print(f\"Target: {s3_target_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. INGESTION LOGIC (PHYSICAL STORAGE)\n",
    "# -------------------------------------------------------------------------\n",
    "def ingest_bronze_layer():\n",
    "    \"\"\"\n",
    "    Reads raw CSV data and persists it to S3 in Delta format.\n",
    "    \n",
    "    Architecture Decision:\n",
    "    - Mode 'FAILFAST': Ensures strict data quality at the gate. If the CSV is malformed, \n",
    "      the pipeline halts immediately rather than ingesting corrupt data.\n",
    "    - Write Mode 'OVERWRITE': Since this handles static reference data (Dimensions), \n",
    "      we replace the dataset entirely to ensure idempotency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"‚è≥ Reading raw data from S3...\")\n",
    "        \n",
    "        # Read Source: strict CSV parsing\n",
    "        df = (\n",
    "            spark.read\n",
    "                 .format(\"csv\")\n",
    "                 .option(\"header\", \"true\")\n",
    "                 # Fail immediately if a row doesn't match the header structure\n",
    "                 .option(\"mode\", \"FAILFAST\") \n",
    "                 .load(s3_source_path)\n",
    "        )\n",
    "\n",
    "        print(f\"Persisting data to Delta Lake (Storage)...\")\n",
    "        \n",
    "        # Write to Storage: Decoupled from Metadata\n",
    "        (\n",
    "            df.write\n",
    "              .format(\"delta\")\n",
    "              .mode(\"overwrite\") \n",
    "              .save(s3_target_path)\n",
    "        )\n",
    "\n",
    "        print(f\"[SUCCESS] Delta Parquet files written to: {s3_target_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ingestion failed for {table_name}.\")\n",
    "        # Re-raise exception to ensure Airflow/Databricks Job marks the task as FAILED\n",
    "        raise e\n",
    "\n",
    "# Execute the ingestion function\n",
    "ingest_bronze_layer()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. METADATA REGISTRATION (UNITY CATALOG)\n",
    "# -------------------------------------------------------------------------\n",
    "# Register the S3 location as a Table in the Metastore.\n",
    "# This separates the \"Compute\" (Spark) from the \"Definition\" (SQL).\n",
    "print(f\"Registering table in Unity Catalog...\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {full_table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{s3_target_path}'\n",
    "    \"\"\")\n",
    "    print(f\"[SUCCESS] External table registered: {full_table_name}\")\n",
    "\n",
    "except AnalysisException as e:\n",
    "    print(f\"[ERROR] Metastore registration failed.\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5057975449237274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_static_data_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
